{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uf2_b8eVUVKS"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and Load Data\n",
        "\n",
        "- Downloads dataset archive using `kagglehub`  \n",
        "- Locates CSV file within the archive  \n",
        "- Loads data into pandas DataFrame  \n",
        "- Prints DataFrame shape and initial rows"
      ],
      "metadata": {
        "id": "hYjSW_fZWqUN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZI0oF3iUVKT",
        "outputId": "01fbf081-dece-4d9e-8a95-6bc15d69cbe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/neelghoshal/reddit-mental-health-data?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.83M/1.83M [00:00<00:00, 103MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Dataset downloaded to: /root/.cache/kagglehub/datasets/neelghoshal/reddit-mental-health-data/versions/1\n",
            "Loading CSV file: /root/.cache/kagglehub/datasets/neelghoshal/reddit-mental-health-data/versions/1/data_to_be_cleansed.csv\n",
            "Dataset loaded successfully!\n",
            "First 5 records:\n",
            "   Unnamed: 0                                               text  \\\n",
            "0           0  Welcome to /r/depression's check-in post - a p...   \n",
            "1           1  We understand that most people who reply immed...   \n",
            "2           2  Anyone else just miss physical touch? I crave ...   \n",
            "3           3  I’m just so ashamed. Everyone and everything f...   \n",
            "4           4  I really need a friend. I don't even have a si...   \n",
            "\n",
            "                                               title  target  \n",
            "0  Regular check-in post, with information about ...       1  \n",
            "1  Our most-broken and least-understood rules is ...       1  \n",
            "2  I haven’t been touched, or even hugged, in so ...       1  \n",
            "3                    Being Depressed is Embarrassing       1  \n",
            "4  I'm desperate for a friend and to feel loved b...       1  \n",
            "\n",
            "Dataset shape: (5957, 4)\n",
            "Columns: ['Unnamed: 0', 'text', 'title', 'target']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "dataset_path = kagglehub.dataset_download(\"neelghoshal/reddit-mental-health-data\")\n",
        "print(f\"Dataset downloaded to: {dataset_path}\")\n",
        "\n",
        "csv_file = None\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.csv'):\n",
        "            csv_file = os.path.join(root, file)\n",
        "            break\n",
        "    if csv_file:\n",
        "        break\n",
        "\n",
        "if csv_file:\n",
        "    print(f\"Loading CSV file: {csv_file}\")\n",
        "    df = pd.read_csv(csv_file)\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "    print(\"First 5 records:\")\n",
        "    print(df.head())\n",
        "    print(f\"\\nDataset shape: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "else:\n",
        "    print(\"No CSV file found in the dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Text and Encode Labels\n",
        "\n",
        "**Text Standardization:**\n",
        "- Removes URLs and artifacts from text data\n",
        "\n",
        "**Label Processing:**\n",
        "- Maps numeric classes to string labels (e.g., `0 → 'Stress'`)\n",
        "- Uses `LabelEncoder` to convert labels to integer IDs\n",
        "\n",
        "**Data Filtering:**\n",
        "- Removes empty or very short text entries"
      ],
      "metadata": {
        "id": "1wZqZUftW3jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Clean and preprocess text data.\"\"\"\n",
        "    if pd.isna(text) or text == \"\":\n",
        "        return \"\"\n",
        "    # Convert to string\n",
        "    text = str(text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "    # Remove Reddit-specific formatting\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove special characters but keep basic punctuation\n",
        "    text = re.sub(r'[^\\w\\s\\.\\!\\?\\,\\;\\:\\-\\(\\)]', '', text)\n",
        "    # Strip whitespace\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Prepare labels\n",
        "target_to_label = {\n",
        "    0: 'Stress',\n",
        "    1: 'Depression',\n",
        "    2: 'Bipolar disorder',\n",
        "    3: 'Personality disorder',\n",
        "    4: 'Anxiety'\n",
        "}\n",
        "\n",
        "df['label'] = df['target'].map(target_to_label)\n",
        "# Drop rows where target was not in our mapping\n",
        "df.dropna(subset=['label'], inplace=True)\n",
        "print(f\"Using 'target' column for labels. Number of samples after dropping unmapped targets: {len(df)}\")\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_id'] = label_encoder.fit_transform(df['label'])\n",
        "print(f\"Label distribution:\")\n",
        "for label, count in df['label'].value_counts().items():\n",
        "    print(f\"  {label}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wubc2qqPJreN",
        "outputId": "2e938ae8-f504-4e2a-f301-52c5dfd2ceb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 'target' column for labels. Number of samples after dropping unmapped targets: 5957\n",
            "Label distribution:\n",
            "  Depression: 1202\n",
            "  Personality disorder: 1201\n",
            "  Anxiety: 1188\n",
            "  Bipolar disorder: 1185\n",
            "  Stress: 1181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data into Train, Validation, and Test Sets\n",
        "\n",
        "**Data Preparation:**\n",
        "- Selects final columns: `text`, `label`, and `label_id`\n",
        "\n",
        "**Stratified Splitting:**\n",
        "- Creates three subsets:\n",
        "  - Training set\n",
        "  - Validation set  \n",
        "  - Test set\n",
        "- Maintains consistent class distribution across all splits"
      ],
      "metadata": {
        "id": "X7gX_MjMXEVz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpHNZE-nUVKU",
        "outputId": "ff68ffd5-8473-4223-9fc4-32cf05141fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning text data...\n",
            "Removed 429 rows with empty/short text after cleaning\n",
            "Creating fine-tuning dataset...\n",
            "Dataset splits:\n",
            "  Train: 3869 samples\n",
            "  Validation: 829 samples\n",
            "  Test: 830 samples\n"
          ]
        }
      ],
      "source": [
        "text_columns = ['text', 'body', 'content', 'post', 'comment', 'title']\n",
        "text_column = None\n",
        "for col in text_columns:\n",
        "    if col in df.columns:\n",
        "        text_column = col\n",
        "        break\n",
        "\n",
        "# Clean text data\n",
        "print(\"Cleaning text data...\")\n",
        "df['cleaned_text'] = df[text_column].apply(clean_text)\n",
        "# Remove rows with empty text after cleaning\n",
        "initial_count = len(df)\n",
        "df = df[df['cleaned_text'].str.len() > 10]\n",
        "print(f\"Removed {initial_count - len(df)} rows with empty/short text after cleaning\")\n",
        "\n",
        "# Create fine-tuning dataset\n",
        "print(\"Creating fine-tuning dataset...\")\n",
        "# Select relevant columns\n",
        "fine_tuning_data = df[['cleaned_text', 'label', 'label_id']].copy()\n",
        "# Rename columns for consistency\n",
        "fine_tuning_data = fine_tuning_data.rename(columns={'cleaned_text': 'text'})\n",
        "# Split into train/validation/test sets\n",
        "if len(fine_tuning_data['label'].unique()) > 1:\n",
        "    train_df, temp_df = train_test_split(fine_tuning_data, test_size=0.3, random_state=42, stratify=fine_tuning_data['label'])\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
        "\n",
        "\n",
        "print(f\"Dataset splits:\")\n",
        "print(f\"  Train: {len(train_df)} samples\")\n",
        "print(f\"  Validation: {len(val_df)} samples\")\n",
        "print(f\"  Test: {len(test_df)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>"
      ],
      "metadata": {
        "id": "0ZdQWxeBOqrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Hugging Face DatasetDict\n",
        "\n",
        "**Conversion:**\n",
        "- Transforms pandas DataFrames into Hugging Face `Dataset` objects  \n",
        "  - Training split  \n",
        "  - Validation split  \n",
        "  - Test split  \n",
        "\n",
        "**Consolidation:**\n",
        "- Combines all splits into a `DatasetDict`  \n",
        "- Prepares data in the standard format for Hugging Face `Trainer` API"
      ],
      "metadata": {
        "id": "R1i3zwU0XVNR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3BySb5aUVKU",
        "outputId": "a872cf64-c1a0-4cc7-8484-2d488bcf7f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset info: {'num_classes': 5, 'class_names': ['Anxiety', 'Bipolar disorder', 'Depression', 'Personality disorder', 'Stress'], 'train_samples': 3869, 'validation_samples': 829, 'test_samples': 830, 'total_samples': 5528}\n",
            "Label mapping: {0: 'Anxiety', 1: 'Bipolar disorder', 2: 'Depression', 3: 'Personality disorder', 4: 'Stress'}\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "dataset = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': val_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "# Create label mapping\n",
        "label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
        "dataset_info = {\n",
        "    \"num_classes\": len(label_encoder.classes_),\n",
        "    \"class_names\": list(label_encoder.classes_),\n",
        "    \"train_samples\": len(train_df),\n",
        "    \"validation_samples\": len(val_df),\n",
        "    \"test_samples\": len(test_df),\n",
        "    \"total_samples\": len(train_df) + len(val_df) + len(test_df)\n",
        "}\n",
        "print(f\"Dataset info: {dataset_info}\")\n",
        "print(f\"Label mapping: {label_mapping}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Pre-trained Model and Tokenizer\n",
        "\n",
        "**Model Setup:**\n",
        "- Architecture: `distilbert-base-uncased`\n",
        "- Customization:\n",
        "  - Replaces head with `AutoModelForSequenceClassification`\n",
        "  - Configures output layer for target class count\n",
        "\n",
        "**Tokenization:**\n",
        "- Loads matching tokenizer\n"
      ],
      "metadata": {
        "id": "WbD61gNrYJNY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ndOc34XUVKV",
        "outputId": "7e3d87d2-aaa0-4da0-f2e0-6b6bd5e6ae8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Model: distilbert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded with 5 output classes\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer and model\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Model: {model_name}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Add padding token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=dataset_info[\"num_classes\"],\n",
        "    ignore_mismatched_sizes=True,\n",
        "    # problem_type=\"single_label_classification\" # Removed as it's only for num_labels=1\n",
        ")\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "print(f\"Model loaded with {dataset_info['num_classes']} output classes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize the Datasets\n",
        "\n",
        "**Tokenization Function:**\n",
        "- Converts raw text → numerical `input_ids`\n",
        "- Handles:\n",
        "  - Padding to uniform length\n",
        "  - Truncation (max_length=512)\n",
        "\n",
        "**Batch Processing:**\n",
        "- Applies to entire `DatasetDict`\n",
        "- Output format: PyTorch tensors\n",
        "\n",
        "**Configuration:**\n",
        "- Fixed sequence length: 512 tokens\n",
        "- Automatic padding/truncation"
      ],
      "metadata": {
        "id": "Qb64XoZGYeKh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149,
          "referenced_widgets": [
            "3863ce10f8be4bd5851a164c08aee50b",
            "ffbe949822c247e894bcd80b152354a7",
            "cb15239954a848108aecfb7a6b57d883",
            "19826c278d794cab9631a8a59d97e3ed",
            "9d7c941b3a9242768e31946cf13d91bb",
            "d67bf5e882f14c739e741cf92e28fcbb",
            "28b6ba4db7fd4dbcaf5f3b2f70d506bc",
            "4b7a206288184225ad4a214e36860c87",
            "b770175eb1f24edcbe4a604c94b6e430",
            "a4b3b59d1b694191be729f4b9b09f727",
            "43a8b72c2c514577bfb0552cdec7103b",
            "33cb1c472e29484b88060d34d60713d3",
            "079d9fde88c847d58de8319924e5c20f",
            "45a82cc708d74b1ab0a38816b3ef8077",
            "a11b67ef6b424a73a507effefdd6abc4",
            "d3a1ac55dbf647e08f552d70ed47ac78",
            "c8cf1c6df673401183cc538f5d2a0b65",
            "3726160e8ad64f3ea7e665b2e4033775",
            "7a496bfbc9214369af92a7604cfe5107",
            "ad9f094d5b5142b284f7446925e42c82",
            "d68c000b32e9414da60e244288d8849b",
            "16367fd86ed84a01a5063059c87000e7",
            "1fb87faa01dd4bb1a46787d0a984ef94",
            "8459c55cf761420eba0dc83d6f19033e",
            "2f2cc9133f1042efa9e10f4b5f17cbca",
            "884fbf7b65724703b8533be2736f2479",
            "3c43797afd514f5983194798a6eea557",
            "274152f17f8844458f36d106379bdbc7",
            "e244d83d40c5444784970dcd50c46e56",
            "3e5a25e823074a878e0fc659777f142c",
            "50d8f9efa1c64924bea54fdeda5030ff",
            "dc71d64c02e1494686118d10860944cc",
            "67c8d42670fc449c9e2bc6aeeeb4c180"
          ]
        },
        "id": "PJOqqWhkUVKV",
        "outputId": "e7fe273f-af3e-4c36-da42-b9c2105d29de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing dataset for training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3869 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3863ce10f8be4bd5851a164c08aee50b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/829 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33cb1c472e29484b88060d34d60713d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/830 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fb87faa01dd4bb1a46787d0a984ef94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset tokenization completed!\n"
          ]
        }
      ],
      "source": [
        "# Tokenize datasets\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the examples and add labels.\"\"\"\n",
        "    tokens = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "    # Use label_id instead of label for integer labels\n",
        "    # For multi-class classification, labels should be shape [batch_size]\n",
        "    tokens[\"labels\"] = examples[\"label_id\"]\n",
        "    return tokens\n",
        "\n",
        "print(\"Preparing dataset for training...\")\n",
        "# Tokenize the dataset\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    # Remove the original 'label' column which contains strings\n",
        "    remove_columns=['label', 'label_id']\n",
        ")\n",
        "# Set format for PyTorch\n",
        "tokenized_dataset.set_format(\"torch\")\n",
        "print(\"Dataset tokenization completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Evaluation Metrics\n",
        "\n",
        "**Functionality:**\n",
        "- Calculates classification metrics for model evaluation\n",
        "- Processes raw model predictions and true labels\n",
        "\n",
        "**Metrics Computed:**\n",
        "- Accuracy\n",
        "- Weighted F1-score  \n",
        "- Weighted Precision  \n",
        "- Weighted Recall  \n",
        "\n",
        "**Key Operations:**  \n",
        "1. Extracts class predictions via argmax  \n",
        "2. Computes multi-class metrics with weighting  \n"
      ],
      "metadata": {
        "id": "S3VQGRDyZKIs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvM6mKgQUVKV"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute metrics for evaluation.\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model\n",
        "\n",
        "**Training Configuration:**\n",
        "- Output directory: `results/fine_tuned_model`\n",
        "- Learning rate: 2e-5\n",
        "- Batch size: 8 (train/eval)\n",
        "- Epochs: 3\n",
        "- Weight decay: 0.01\n",
        "- FP16 acceleration\n",
        "\n",
        "**Training Process:**\n",
        "- Evaluation after each epoch\n",
        "- Model checkpointing per epoch\n",
        "- Best model retention\n",
        "- Gradient accumulation (steps: 4)\n",
        "- Warmup steps: 500\n",
        "\n",
        "**Monitoring:**\n",
        "- Logging every 100 steps\n",
        "- Disabled external reporting"
      ],
      "metadata": {
        "id": "ExfE_axeZYZu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "LvEo-n1SUVKV",
        "outputId": "4bdbec07-a2b0-4706-d295-fd08c68b2e4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "Training started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2238348422.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='363' max='363' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [363/363 03:05, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.603800</td>\n",
              "      <td>1.563548</td>\n",
              "      <td>0.366707</td>\n",
              "      <td>0.305483</td>\n",
              "      <td>0.526777</td>\n",
              "      <td>0.366707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.494500</td>\n",
              "      <td>1.114819</td>\n",
              "      <td>0.639324</td>\n",
              "      <td>0.643294</td>\n",
              "      <td>0.670544</td>\n",
              "      <td>0.639324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.098400</td>\n",
              "      <td>0.780678</td>\n",
              "      <td>0.733414</td>\n",
              "      <td>0.732510</td>\n",
              "      <td>0.747913</td>\n",
              "      <td>0.733414</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "print(\"Starting model training...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"results/fine_tuned_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        "    logging_dir=\"results/fine_tuned_model/logs\",\n",
        "    logging_steps=100,\n",
        "    save_total_limit=2,\n",
        "    dataloader_num_workers=2,\n",
        "    warmup_steps=500,\n",
        "    gradient_accumulation_steps=4,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=[],\n",
        "    run_name=None,\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Training started...\")\n",
        "trainer.train()\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Completed 3 epochs in ~3 minutes\n",
        "- Steady improvement across all metrics\n",
        "- 100% increase in accuracy (36.67% → 73.34%)\n",
        "- Validation loss decreased by 50% (1.5635 → 0.7807)\n",
        "- Healthy convergence pattern (no overfitting)\n"
      ],
      "metadata": {
        "id": "yGz6nVqhZqGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<hr>"
      ],
      "metadata": {
        "id": "FmnJIT3fZ7Qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Model on Test Set"
      ],
      "metadata": {
        "id": "ngPqr1t0aGJB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdLOXG2KUVKV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "54c5975d-1d77-4f85-f819-04147b7882db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='104' max='104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [104/104 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results:\n",
            "  eval_loss: 0.8708\n",
            "  eval_accuracy: 0.6867\n",
            "  eval_f1: 0.6863\n",
            "  eval_precision: 0.6979\n",
            "  eval_recall: 0.6867\n",
            "  eval_runtime: 3.4634\n",
            "  eval_samples_per_second: 239.6500\n",
            "  eval_steps_per_second: 30.0280\n",
            "  epoch: 3.0000\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "print(\"Evaluating model on test set...\")\n",
        "# Evaluate on test set\n",
        "results = trainer.evaluate(tokenized_dataset[\"test\"])\n",
        "print(\"Test Results:\")\n",
        "for key, value in results.items():\n",
        "    print(f\"  {key}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CggOczd5UVKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f3e6506-901e-4ead-e11d-ae583ca4fa57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Sample Predictions:\n",
            "Text: I've been feeling really down lately and can't see...\n",
            "Prediction: Depression (confidence: 0.450)\n",
            "----------------------------------------\n",
            "Text: My anxiety is through the roof today, I can't stop...\n",
            "Prediction: Anxiety (confidence: 0.816)\n",
            "----------------------------------------\n",
            "Text: I'm having trouble sleeping and my thoughts are ra...\n",
            "Prediction: Anxiety (confidence: 0.384)\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Make sample predictions\n",
        "def predict_sample(text):\n",
        "    \"\"\"Make a prediction on a sample text.\"\"\"\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    # Move inputs to device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "        confidence = predictions[0][predicted_class].item()\n",
        "    # Get label name\n",
        "    predicted_label = label_mapping[predicted_class] # Access using integer key\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"predicted_label\": predicted_label,\n",
        "        \"predicted_class\": predicted_class,\n",
        "        \"confidence\": confidence,\n",
        "        \"all_probabilities\": predictions[0].cpu().numpy().tolist()\n",
        "    }\n",
        "\n",
        "print(\"\\n Sample Predictions:\")\n",
        "sample_texts = [\n",
        "    \"I've been feeling really down lately and can't seem to get out of bed.\",\n",
        "    \"My anxiety is through the roof today, I can't stop worrying about everything.\",\n",
        "    \"I'm having trouble sleeping and my thoughts are racing constantly.\"\n",
        "]\n",
        "\n",
        "for text in sample_texts:\n",
        "    prediction = predict_sample(text)\n",
        "    print(f\"Text: {text[:50]}...\")\n",
        "    print(f\"Prediction: {prediction['predicted_label']} (confidence: {prediction['confidence']:.3f})\")\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Hyperparameters\n",
        "\n",
        "- **learning_rate**: Step size for gradient descent. Small = stable, slow; large = fast, risky. *(2e-5)*\n",
        "- **train_batch_size**: Batch size per device. Larger = stable gradients, more memory. *(8)*\n",
        "- **num_train_epochs**: Training cycles over data. More = better fit, risk of overfitting. *(3)*\n",
        "- **weight_decay**: L2 regularization to prevent overfitting. *(0.01)*\n",
        "- **warmup_steps**: Gradual LR increase to stabilize early training. *(500)*\n",
        "- **gradient_accumulation_steps**: Combines gradients over steps to mimic larger batch size. *(4)*\n",
        "\n",
        "---\n",
        "\n",
        "## Additional Tuning Options\n",
        "\n",
        "- **max_length**: Max token length. Short = faster, less info; long = more context, higher cost. *(512)*\n",
        "- **dropout rate**: Controls regularization; reduces overfitting.\n",
        "- **LR scheduler**: Strategy for adjusting learning rate (e.g., linear, cosine).\n"
      ],
      "metadata": {
        "id": "nXg31CuTLiwP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6879b5a"
      },
      "source": [
        "hyperparameter_ranges = {\n",
        "    \"learning_rate\": [1e-5, 2e-5, 3e-5, 5e-5],\n",
        "    \"per_device_train_batch_size\": [8, 16, 32],\n",
        "    \"num_train_epochs\": [3, 5, 10],\n",
        "    \"weight_decay\": [0.0, 0.01, 0.1],\n",
        "    \"warmup_steps\": [0, 100, 500],\n",
        "    \"gradient_accumulation_steps\": [1, 2, 4, 8],\n",
        "    \"max_length\": [256, 512],\n",
        "    # Dropout can be tuned, usually in model config, default is often 0.1 for DistilBERT\n",
        "    # \"dropout\": [0.1, 0.2, 0.3], # If we were tuning model config directly\n",
        "    # Learning rate scheduler type is another option, default is 'linear'\n",
        "    # \"lr_scheduler_type\": [\"linear\", \"cosine\"],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning with Optuna\n",
        "\n",
        "### Optimization Strategy\n",
        "- **Search Space**:\n",
        "  - Learning rate: 1e-5 to 5e-5 (log scale)\n",
        "  - Batch sizes: [8, 16, 32]\n",
        "  - Epochs: [3, 5, 10]\n",
        "  - Sequence lengths: [256, 512]\n",
        "  \n",
        "- **Optimization Target**: Maximize validation accuracy\n",
        "\n",
        "### Key Features\n",
        "- **Automatic Exploration**: Tests 5 different configurations\n",
        "- **Fresh Start**: Reinitializes model weights for each trial\n",
        "- **Efficient Setup**:\n",
        "  - Disables checkpointing during search\n",
        "  - Uses GPU acceleration when available\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uk_SQNX0bVIN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "509764df93e1441ea24e403174baeafc",
            "b32bd0b7c70b42f997b4bbf56a3fdbea",
            "94699e2434ec4e64b81ffb6aec7cadd9",
            "75ded3d8c0a446c8a7578501e1009fd8",
            "d2163d9ae8f04ef19791a09bb2a2a8b1",
            "c64246eecd5947acb3942828432bfdce",
            "bb775d7aa2314a779abf02ae442b37cc",
            "02b90dc4848446528f4fe45f5e8438de",
            "59f70a06617c41a3854e18f8e33e2439",
            "31051fa307414fd7b3e9f7de2f2cde21",
            "09ce679fcc94464b94ed0ff065da3038",
            "057825f05cbc4da29640f696f5004e1c",
            "5b18b836c34b4da0a2a8cc0cfa52f56d",
            "7a68d6e2f8f349008d434b90d7edcb92",
            "6ba2e24b746544aba255615e5f1916a7",
            "c1aab55c1bdf4bfb8dd03f3fef71a747",
            "bff5bca04c5948d19450dd74a4afd6f7",
            "6c46119036474571912002a7be8f5b2a",
            "4775765219b6436a8f1beaaaa17c5403",
            "6fcf1456a6d44adcb05a6ba53dfd5d92",
            "d64ec993728345e481b3f5fbc721b8d0",
            "bafda05a4563486c96f3c6355ac2d5ab",
            "332a1845b0f14a048e2e8a2e7b878872",
            "098be072d9fb4e37a92b83f4426b7970",
            "c31ea0c7373b43d1833e364c0a37bb2f",
            "6c71e1381ffd40b9bf8ba8b62fffce32",
            "152e7e4dd8fc49999de8075c734bcc9d",
            "3214a3f6e40e4d03871c1dcfe1a6c88f",
            "b31f255080cc402fa94b6aef9d3b148b",
            "8bcd520b4a89490396d0d8600ffa55e2",
            "78024245547141dc800b5bf38667c71f",
            "90b740b27aa749b99b6539304d9cba0e",
            "269368edccc240f4bf8fa9ce74a998a2",
            "2b2d74cb711143b594bb93be78ee8035",
            "3cb0e500c7bb41cc8a1b4e4882e39da8",
            "4bdee8c3c50048dcb4be5d4844916b3d",
            "ac32e2be63474b5c8fec8d714483f438",
            "0fca234582384fc48524bd4e20b83d31",
            "92057d77a1524626a1c01559dc7d5d31",
            "a4e6cc16f5404059906a6f7c20d92a8e",
            "ca9c830156634d67a4594732d3b87849",
            "48f5684a7d704ee2a42b25f1670e6564",
            "67dea0348c9c47b8abd95eef46755ee1",
            "5d44aa11c065450a960c83265807c7ed",
            "401ad3ea0b8a4136bb9301a1f87c44d1",
            "0fafceb86c4748058ba6a4735a64e360",
            "0b287ff4b0a34f1ca78cea27a8b92390",
            "cbd2c7d437d448bfba84ae78c9cf574b",
            "56f3ede5fef84026a17dde9c290487c4",
            "d4e9bbcec8204e108da50372ba54ddba",
            "c4cf8e97617a42278254de146c7347e1",
            "3bb170c3d75544f2ac6fc7997542988c",
            "ee5fd0c5f47845b89a6a00df90d5d039",
            "0e0be9c3eb5d4afe86ac0a35c3acd6b2",
            "a3bf6314dfb14bf496eec7b3327229cb",
            "2bcb8f63ab504ab08ce42d7f0ff7f71e",
            "cea3a7da24eb4b6a838665961f0bef88",
            "789a637e9b0a415cb5da8dfe4b9dcdfc",
            "83e31c586b9d43a39b83a89dc31e7d2d",
            "0ebbac20c45d4e3bbc20d90a77a3b104",
            "f9cca52295db4dcc817d2530eb3a47a5",
            "0df5563c565644fba32762137def0516",
            "cbe8647199bf4a73b550f8b3127f878f",
            "dc74b8597c56413e83b21c98b8e8e529",
            "9a3dfbce1d504e3aafb932aeb21667de",
            "5712fb2ef0ab49289fb9aef473e6d881",
            "a60e2fc39bfd43fb8249a062fd8da4c0",
            "df11e78f93594f6b958471e126638117",
            "3519b84c6cf7443ca7b8692ac3b139a8",
            "0a6ca759294e4dc9a824b2d6f4d1b04c",
            "b039f49ad11d49c1a7ef42e4745837a4",
            "9458835534eb4b27a9ca216f2cfbfc5f",
            "ac16fc47059c4f9e96fd1b193c63b790",
            "480d99fb17ff4d489c7675e6148082ec",
            "51a87402c096403395f4668f8e8322e8",
            "4732ecc7f7ed41388bbc41b24258e1b8",
            "cde21a1b9b504b1abd722364df731837",
            "600f7640f5a64490bcf3cbecdc521e43",
            "5c06bbe24a8740f2bf6eece735dd5630",
            "23fc3e33a83c4a369e9f617ac4c90ec7",
            "866e138dc0de414993dd68e14dd30646",
            "3279eefe4b65454198e18692ad817046",
            "7451ec8d84d54897a6cd3a94399bbc5d",
            "295699af70fb48af8f3dcd882dea251b",
            "4eb6fa0ae0434f06953cfab33b05dc3f",
            "c05801a2baf047e69936aa817472106b",
            "bc3a32579a4348d181741a880dfdce2c",
            "51ea095ef6e6493da692a0d538f0520f",
            "89147253a5484ec9a9c4b879c10b0300",
            "cbc18403a5654b80a93064e5386d9c8c",
            "f0c8054c09a04bf0be3dc450c484fee3",
            "ed705a0ef31a4057bfd3f4481e51cfce",
            "5d4faac594aa4b509464fc234009895f",
            "1b041d4fb29543f18a7a129631b959da",
            "a9ef240eeb8443888e4baff9a8e1125a",
            "f3c3ab997a0d42d289672b33c841b20a",
            "94ef65c796fe4224a5f376a6fbd5832b",
            "b2bd4151987c419f99d090d0d176f1c1",
            "94b2609519494908a47c47668caf26f4",
            "9514c549fd8e42c2b7b2879e82b124c3",
            "17306e13b8dc4dc09908c79489f7a61e",
            "7582f809a07c4e0e815d825929f6769c",
            "7504d8c01b1b43b2a40eb388f2d9e811",
            "090b55de310546c0ac2b14efecf2cbb3",
            "b4b1f62c2a00434eac0a7e21c6c2e7ca",
            "e6fb8b98353e4fd4acf9894a692ff0ef",
            "f681456ac73b484abbe94c4530683e20",
            "ed077ca8656844c5a4fe29205a678bd9",
            "e03e1306fb5e46b0830628ff0c2d6c2c",
            "10ec81f792cd4ab3a2a38a3810eb375d",
            "e1619b65321a48e08e8cebdd7f6578f4",
            "f69cd7c1f80f42cba88b53365e1dc3b0",
            "8aca488cfbb24cd7a4701a9107c17245",
            "31d30e62a49b463c900075435ae8150a",
            "7fbfae92a84643bab126431b6287f210",
            "09542091005147acb1b6897ca63f8322",
            "38e26b59dd5a4893953795d7aaefd474",
            "0eafa6abb5174bd3ba7927f75a1b79de",
            "42baf0d4aec24e248997a88d45741f8c",
            "93b02cee4d4241a7ae218b7fb6810da4",
            "e15f6313b87f4b80895014804709f115",
            "039d86379147406ea97421695f7112a3",
            "1608ba65be1a4dba99c4a082fa8398ad",
            "eb500a0a4c5d430cbfc52f999e618290",
            "14597dae254041089d8e56c8431a2b64",
            "eae09b3f6c024ed1aaa483acd141982e",
            "5d3190f11e6245118efd1470b33db363",
            "f1cbf014a75a4fe882ff5dc976497b18",
            "13afb92eee814d1484c3f57668242c74",
            "fc9897edd4fb42a38992c5c5746ab727",
            "40e6ac579c1348bfb7d0bf9ac8a9679a",
            "bd614088e987452b9cdee00209b213ef",
            "b69b7e7efe14404eb1a520a104ca3324",
            "ab4e9a419b8b40659941d16f0546c6f3",
            "fb27526bc54a4190be3d52cab7d70cb6",
            "6db9a40cf27b411e8cc1ff083d0d0450",
            "d4acfa8b76c7435d8be80ca75adbed0a",
            "5b6ec84c7e46424685004e4c25fe3fa8",
            "214fe3efe3a54029ab002b23b08fc652",
            "ccc4bc28a1ef44caaecc186689ce3a34",
            "6678d56ba554463dade0b249842359a8",
            "19453f4b374d451492018a1bd5a0a560",
            "6b95dedce3d54e46b02a0ddabcd2fc13",
            "1fb7fa57284340adb0ccacb1e9f7b3c7",
            "8f258039e0e54fe783340dbcd01b829f",
            "e4928906b7104d4399af1c70355d3163",
            "687954ee1ff04c6eb29367e631481175",
            "cebf52a220eb461c930c8fcdf941f404",
            "e4de28a019f041f0848b80c52cd4c655",
            "41d82ceac59d4957bcd15a75102a5ae4",
            "6bb3e1c88c164f889e00359f4cfcddac",
            "08bf39b856a44fce8834a049979631bb",
            "96b08a4a0b0e49909fd677d7154ed951",
            "07140af8792b4d20be8e3db0ea2d3eb3",
            "7b6cfba2fb35451fa314d82f8a4bf42a",
            "8964fe1158e94550abed425052a1e571",
            "c3abc5fc2ba841539009cb1ef5403256",
            "0cf187ab1f9b42afa635ae54809a9734",
            "677ce485b1fa492b9d5996d35b9d7306",
            "4442940c444b4e0ab1478bdb12200b09",
            "733b3472614d473fa98eb6220d320aab",
            "614ae8c2daa440aab2a848e71eac503e",
            "14abf87f8d9545409fb6a5c3279543eb",
            "bab21c07021e4275a4d42cf4b1fd1f9f",
            "c739ae8dd3e646708b1b004680720dfa"
          ]
        },
        "id": "c2aee8b6",
        "outputId": "832b8161-6ee2-48db-bf32-93d4677e3ce9"
      },
      "source": [
        "import optuna\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
        "    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32])\n",
        "    num_train_epochs = trial.suggest_categorical(\"num_train_epochs\", [3, 5, 10])\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", [0.0, 0.01, 0.1])\n",
        "    warmup_steps = trial.suggest_categorical(\"warmup_steps\", [0, 100, 500])\n",
        "    gradient_accumulation_steps = trial.suggest_categorical(\"gradient_accumulation_steps\", [1, 2, 4, 8])\n",
        "    max_length = trial.suggest_categorical(\"max_length\", [256, 512])\n",
        "\n",
        "    # Re-tokenize dataset with suggested max_length\n",
        "    def tokenize_and_format(examples):\n",
        "        tokens = tokenizer(\n",
        "            examples[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        )\n",
        "        tokens[\"labels\"] = examples[\"label_id\"]\n",
        "        return tokens\n",
        "\n",
        "    tokenized_dataset_trial = dataset.map(\n",
        "        tokenize_and_format,\n",
        "        batched=True,\n",
        "        remove_columns=['label', 'label_id', 'text']\n",
        "    )\n",
        "    tokenized_dataset_trial.set_format(\"torch\")\n",
        "\n",
        "\n",
        "    # Create TrainingArguments with suggested hyperparameters\n",
        "    training_args_trial = TrainingArguments(\n",
        "        output_dir=f\"results/optuna_trial_{trial.number}\",\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        weight_decay=weight_decay,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        load_best_model_at_end=False,\n",
        "        push_to_hub=False,\n",
        "        logging_dir=f\"results/optuna_trial_{trial.number}/logs\",\n",
        "        logging_steps=100,\n",
        "        save_total_limit=0,\n",
        "        dataloader_num_workers=2,\n",
        "        warmup_steps=warmup_steps,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        report_to=[],\n",
        "    )\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # Re-initialize the model for each trial to reset weights\n",
        "    model_trial = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=dataset_info[\"num_classes\"],\n",
        "        ignore_mismatched_sizes=True,\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model_trial,\n",
        "        args=training_args_trial,\n",
        "        train_dataset=tokenized_dataset_trial[\"train\"],\n",
        "        eval_dataset=tokenized_dataset_trial[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    # Return the metric to optimize\n",
        "    return eval_results[\"eval_accuracy\"]\n",
        "\n",
        "# Create an Optuna study\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "\n",
        "# Run the optimization\n",
        "print(\"Starting Optuna hyperparameter tuning...\")\n",
        "study.optimize(objective, n_trials=5)\n",
        "\n",
        "print(\"Optuna hyperparameter tuning completed!\")\n",
        "print(\"Best hyperparameters:\", study.best_params)\n",
        "print(\"Best accuracy:\", study.best_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-02 16:29:54,884] A new study created in memory with name: no-name-47c9fae6-98dc-4cc4-be5c-1dd8e02fb882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Optuna hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3869 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "509764df93e1441ea24e403174baeafc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/829 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "057825f05cbc4da29640f696f5004e1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/830 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "332a1845b0f14a048e2e8a2e7b878872"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-1463093359.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1210' max='1210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1210/1210 08:57, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.500600</td>\n",
              "      <td>0.892206</td>\n",
              "      <td>0.703257</td>\n",
              "      <td>0.702218</td>\n",
              "      <td>0.708946</td>\n",
              "      <td>0.703257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.833400</td>\n",
              "      <td>0.701937</td>\n",
              "      <td>0.756333</td>\n",
              "      <td>0.757280</td>\n",
              "      <td>0.761491</td>\n",
              "      <td>0.756333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.609000</td>\n",
              "      <td>0.677040</td>\n",
              "      <td>0.786490</td>\n",
              "      <td>0.786917</td>\n",
              "      <td>0.797345</td>\n",
              "      <td>0.786490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.413100</td>\n",
              "      <td>0.704916</td>\n",
              "      <td>0.784077</td>\n",
              "      <td>0.783225</td>\n",
              "      <td>0.787509</td>\n",
              "      <td>0.784077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.216700</td>\n",
              "      <td>0.744367</td>\n",
              "      <td>0.788902</td>\n",
              "      <td>0.787895</td>\n",
              "      <td>0.791117</td>\n",
              "      <td>0.788902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.140500</td>\n",
              "      <td>0.771374</td>\n",
              "      <td>0.792521</td>\n",
              "      <td>0.792287</td>\n",
              "      <td>0.798167</td>\n",
              "      <td>0.792521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.106900</td>\n",
              "      <td>0.801404</td>\n",
              "      <td>0.802171</td>\n",
              "      <td>0.802899</td>\n",
              "      <td>0.805447</td>\n",
              "      <td>0.802171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.073100</td>\n",
              "      <td>0.858886</td>\n",
              "      <td>0.794934</td>\n",
              "      <td>0.795367</td>\n",
              "      <td>0.798297</td>\n",
              "      <td>0.794934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.081100</td>\n",
              "      <td>0.874896</td>\n",
              "      <td>0.803378</td>\n",
              "      <td>0.804244</td>\n",
              "      <td>0.807707</td>\n",
              "      <td>0.803378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.040300</td>\n",
              "      <td>0.874570</td>\n",
              "      <td>0.804584</td>\n",
              "      <td>0.804323</td>\n",
              "      <td>0.805925</td>\n",
              "      <td>0.804584</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='104' max='104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [104/104 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-02 16:39:03,829] Trial 0 finished with value: 0.804583835946924 and parameters: {'learning_rate': 2.9337206417264026e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 10, 'weight_decay': 0.0, 'warmup_steps': 100, 'gradient_accumulation_steps': 4, 'max_length': 512}. Best is trial 0 with value: 0.804583835946924.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3869 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b2d74cb711143b594bb93be78ee8035"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/829 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "401ad3ea0b8a4136bb9301a1f87c44d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/830 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bcb8f63ab504ab08ce42d7f0ff7f71e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-1463093359.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='310' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [310/310 04:06, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.188732</td>\n",
              "      <td>0.570567</td>\n",
              "      <td>0.566712</td>\n",
              "      <td>0.582951</td>\n",
              "      <td>0.570567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.838577</td>\n",
              "      <td>0.717732</td>\n",
              "      <td>0.717830</td>\n",
              "      <td>0.737943</td>\n",
              "      <td>0.717732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.728124</td>\n",
              "      <td>0.756333</td>\n",
              "      <td>0.757965</td>\n",
              "      <td>0.773200</td>\n",
              "      <td>0.756333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.977500</td>\n",
              "      <td>0.691867</td>\n",
              "      <td>0.778046</td>\n",
              "      <td>0.777377</td>\n",
              "      <td>0.783124</td>\n",
              "      <td>0.778046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.977500</td>\n",
              "      <td>0.638792</td>\n",
              "      <td>0.788902</td>\n",
              "      <td>0.789746</td>\n",
              "      <td>0.794408</td>\n",
              "      <td>0.788902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.977500</td>\n",
              "      <td>0.660394</td>\n",
              "      <td>0.792521</td>\n",
              "      <td>0.792705</td>\n",
              "      <td>0.798364</td>\n",
              "      <td>0.792521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.366300</td>\n",
              "      <td>0.650908</td>\n",
              "      <td>0.797346</td>\n",
              "      <td>0.797714</td>\n",
              "      <td>0.799574</td>\n",
              "      <td>0.797346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.366300</td>\n",
              "      <td>0.654180</td>\n",
              "      <td>0.796140</td>\n",
              "      <td>0.796480</td>\n",
              "      <td>0.798266</td>\n",
              "      <td>0.796140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.366300</td>\n",
              "      <td>0.663254</td>\n",
              "      <td>0.797346</td>\n",
              "      <td>0.797468</td>\n",
              "      <td>0.799691</td>\n",
              "      <td>0.797346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.161500</td>\n",
              "      <td>0.662546</td>\n",
              "      <td>0.796140</td>\n",
              "      <td>0.796337</td>\n",
              "      <td>0.798467</td>\n",
              "      <td>0.796140</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='104' max='104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [104/104 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-02 16:43:17,033] Trial 1 finished with value: 0.7961399276236429 and parameters: {'learning_rate': 4.167868320377578e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 10, 'weight_decay': 0.1, 'warmup_steps': 0, 'gradient_accumulation_steps': 8, 'max_length': 256}. Best is trial 0 with value: 0.804583835946924.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3869 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a60e2fc39bfd43fb8249a062fd8da4c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/829 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "600f7640f5a64490bcf3cbecdc521e43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/830 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89147253a5484ec9a9c4b879c10b0300"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-1463093359.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1210' max='1210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1210/1210 04:29, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.289500</td>\n",
              "      <td>0.913641</td>\n",
              "      <td>0.696019</td>\n",
              "      <td>0.693764</td>\n",
              "      <td>0.700201</td>\n",
              "      <td>0.696019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.778500</td>\n",
              "      <td>0.760067</td>\n",
              "      <td>0.732207</td>\n",
              "      <td>0.731962</td>\n",
              "      <td>0.743747</td>\n",
              "      <td>0.732207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.533000</td>\n",
              "      <td>0.685517</td>\n",
              "      <td>0.772014</td>\n",
              "      <td>0.771789</td>\n",
              "      <td>0.775808</td>\n",
              "      <td>0.772014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.423100</td>\n",
              "      <td>0.685362</td>\n",
              "      <td>0.764777</td>\n",
              "      <td>0.764770</td>\n",
              "      <td>0.768071</td>\n",
              "      <td>0.764777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.372700</td>\n",
              "      <td>0.670896</td>\n",
              "      <td>0.778046</td>\n",
              "      <td>0.778454</td>\n",
              "      <td>0.780518</td>\n",
              "      <td>0.778046</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='104' max='104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [104/104 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-02 16:47:56,618] Trial 2 finished with value: 0.7780458383594693 and parameters: {'learning_rate': 1.5168115145221602e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 5, 'weight_decay': 0.0, 'warmup_steps': 100, 'gradient_accumulation_steps': 1, 'max_length': 512}. Best is trial 0 with value: 0.804583835946924.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3869 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9514c549fd8e42c2b7b2879e82b124c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/829 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1619b65321a48e08e8cebdd7f6578f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/830 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "039d86379147406ea97421695f7112a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-1463093359.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='183' max='183' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [183/183 01:24, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.563968</td>\n",
              "      <td>0.340169</td>\n",
              "      <td>0.287150</td>\n",
              "      <td>0.423721</td>\n",
              "      <td>0.340169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.544700</td>\n",
              "      <td>1.168276</td>\n",
              "      <td>0.599517</td>\n",
              "      <td>0.603091</td>\n",
              "      <td>0.634048</td>\n",
              "      <td>0.599517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.544700</td>\n",
              "      <td>0.944488</td>\n",
              "      <td>0.702051</td>\n",
              "      <td>0.702434</td>\n",
              "      <td>0.702917</td>\n",
              "      <td>0.702051</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='104' max='104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [104/104 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-02 16:49:27,686] Trial 3 finished with value: 0.7020506634499397 and parameters: {'learning_rate': 1.3192294743994892e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 3, 'weight_decay': 0.1, 'warmup_steps': 100, 'gradient_accumulation_steps': 8, 'max_length': 256}. Best is trial 0 with value: 0.804583835946924.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3869 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b69b7e7efe14404eb1a520a104ca3324"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/829 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fb7fa57284340adb0ccacb1e9f7b3c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/830 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b6cfba2fb35451fa314d82f8a4bf42a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-1463093359.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='310' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [310/310 03:53, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.575163</td>\n",
              "      <td>0.347407</td>\n",
              "      <td>0.277120</td>\n",
              "      <td>0.455302</td>\n",
              "      <td>0.347407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.206629</td>\n",
              "      <td>0.548854</td>\n",
              "      <td>0.546936</td>\n",
              "      <td>0.600321</td>\n",
              "      <td>0.548854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.783899</td>\n",
              "      <td>0.738239</td>\n",
              "      <td>0.737006</td>\n",
              "      <td>0.746027</td>\n",
              "      <td>0.738239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.247100</td>\n",
              "      <td>0.711277</td>\n",
              "      <td>0.761158</td>\n",
              "      <td>0.761962</td>\n",
              "      <td>0.768478</td>\n",
              "      <td>0.761158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.247100</td>\n",
              "      <td>0.662597</td>\n",
              "      <td>0.780458</td>\n",
              "      <td>0.779407</td>\n",
              "      <td>0.782893</td>\n",
              "      <td>0.780458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.247100</td>\n",
              "      <td>0.671772</td>\n",
              "      <td>0.791315</td>\n",
              "      <td>0.793372</td>\n",
              "      <td>0.801926</td>\n",
              "      <td>0.791315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.392300</td>\n",
              "      <td>0.648526</td>\n",
              "      <td>0.802171</td>\n",
              "      <td>0.801408</td>\n",
              "      <td>0.802684</td>\n",
              "      <td>0.802171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.392300</td>\n",
              "      <td>0.674702</td>\n",
              "      <td>0.805790</td>\n",
              "      <td>0.807169</td>\n",
              "      <td>0.814350</td>\n",
              "      <td>0.805790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.392300</td>\n",
              "      <td>0.666210</td>\n",
              "      <td>0.810615</td>\n",
              "      <td>0.810715</td>\n",
              "      <td>0.811776</td>\n",
              "      <td>0.810615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.127400</td>\n",
              "      <td>0.671981</td>\n",
              "      <td>0.811821</td>\n",
              "      <td>0.811802</td>\n",
              "      <td>0.814081</td>\n",
              "      <td>0.811821</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='104' max='104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [104/104 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-02 16:53:30,239] Trial 4 finished with value: 0.8118214716525934 and parameters: {'learning_rate': 4.498868200475396e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 10, 'weight_decay': 0.1, 'warmup_steps': 100, 'gradient_accumulation_steps': 4, 'max_length': 256}. Best is trial 4 with value: 0.8118214716525934.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optuna hyperparameter tuning completed!\n",
            "Best hyperparameters: {'learning_rate': 4.498868200475396e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 10, 'weight_decay': 0.1, 'warmup_steps': 100, 'gradient_accumulation_steps': 4, 'max_length': 256}\n",
            "Best accuracy: 0.8118214716525934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Best Configuration Achieved 81.2% Accuracy**  \n",
        "*(Significant improvement from initial 73.3%)*\n",
        "<br>\n",
        "<br>\n",
        "<hr>\n"
      ],
      "metadata": {
        "id": "u-6DSk1Pb0SN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Model Training\n",
        "\n"
      ],
      "metadata": {
        "id": "HB4yf-OmcYmZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684,
          "referenced_widgets": [
            "cbfc4ec1b3c247008cde4aa9c66c98e5",
            "ae2d0141d34541e68ff1fb29d44e4c7a",
            "73dba1be264e4a9cacdc99ab5993c52d",
            "aaa797df97f648518bd4279414de3cf1",
            "8c7b9022234346d0baaa72b269c78277",
            "9b7e044273c64ba7b2ef266952efc655",
            "a0a38a26c4ec4aabb28dfb163003d51e",
            "6f2d48de65a142d78092bbecab4b4e4f",
            "79f26a1cf5414d4c90953fe2568b7c61",
            "0401942da0ca47789eba32c064e8cab9",
            "65a1ebe0d84f4482b6db354d8c164ca7",
            "b7e34e0b0bea415ea7e30d4bc4f91707",
            "4f6fc5f88eff47b8b1656eae8e13f457",
            "494784247db94bd4a56e9f0079ad53a8",
            "03fc7db5fadd40418920e995157f8f27",
            "01eeaff5c47b4adda3d282b77ed1ec42",
            "f14a1cba6b784d4492a5ceb9cfc65d2e",
            "d3465922d39a4bc6b7312b016ce025f5",
            "c6b6ad8382b447e6a7e36e62eabd4417",
            "bee2c3b4338b4432b5a05e2c5085babe",
            "c62d6564776648cda14e9c8dd520ff2a",
            "d8fc0042aa0b4ed9be8f9c73ebef3048",
            "9cb5514369c44e04b411ad3f296e2b6e",
            "c94edc37ba7548b783bd2391ec9bd0cb",
            "87d52148c742434388b2c9ebd948fe9f",
            "76db7ef8bebe4c5a9a859d4bfa3e9632",
            "1788557e820e46d1b09a2afc9ffa69ef",
            "b4556761f5d243efb3fdee21f9f1663b",
            "8fe7ecebc6e240bcbe168e84a637344e",
            "9b935fb3059f4629be219014c7b8a434",
            "6747a8e18208442487ee5eec03cd59ec",
            "c357966e6bae416e9946c9430d901dca",
            "3f40f1c80c364b4da4dbb62b01ab1bcf"
          ]
        },
        "id": "8d1d6124",
        "outputId": "44a125dd-945a-46ae-adad-f8bfae93d064"
      },
      "source": [
        "print(\"Training final model with best hyperparameters...\")\n",
        "\n",
        "# Get best hyperparameters from Optuna study\n",
        "best_params = study.best_params\n",
        "\n",
        "# Define TrainingArguments with best hyperparameters\n",
        "training_args_best = TrainingArguments(\n",
        "    output_dir=\"results/final_model\",  # Output directory for the final model\n",
        "    learning_rate=best_params[\"learning_rate\"],\n",
        "    per_device_train_batch_size=best_params[\"per_device_train_batch_size\"],\n",
        "    num_train_epochs=best_params[\"num_train_epochs\"],\n",
        "    weight_decay=best_params[\"weight_decay\"],\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",  # Save checkpoints every epoch\n",
        "    load_best_model_at_end=True, # Load the best model based on validation metric\n",
        "    push_to_hub=False,\n",
        "    logging_dir=\"results/final_model/logs\",\n",
        "    logging_steps=100,\n",
        "    save_total_limit=2, # Keep only the best and latest model checkpoints\n",
        "    dataloader_num_workers=2,\n",
        "    warmup_steps=best_params[\"warmup_steps\"],\n",
        "    gradient_accumulation_steps=best_params[\"gradient_accumulation_steps\"],\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=[],\n",
        "    run_name=\"final_model_training\",\n",
        ")\n",
        "\n",
        "# Re-initialize the model to train from scratch with the best hyperparameters\n",
        "# Use the original model_name and number of classes\n",
        "model_best = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=dataset_info[\"num_classes\"],\n",
        "    ignore_mismatched_sizes=True,\n",
        ").to(device)\n",
        "\n",
        "# Re-tokenize the dataset using the best max_length found by Optuna for the final training\n",
        "best_max_length = best_params[\"max_length\"]\n",
        "print(f\"Retokenizing full dataset with best max_length: {best_max_length}\")\n",
        "\n",
        "def tokenize_and_format_final(examples):\n",
        "    tokens = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=best_max_length\n",
        "    )\n",
        "    tokens[\"labels\"] = examples[\"label_id\"]\n",
        "    return tokens\n",
        "\n",
        "tokenized_dataset_final = dataset.map(\n",
        "    tokenize_and_format_final,\n",
        "    batched=True,\n",
        "    remove_columns=['label', 'label_id', 'text']\n",
        ")\n",
        "tokenized_dataset_final.set_format(\"torch\")\n",
        "\n",
        "\n",
        "# Initialize trainer with the re-initialized model and best arguments\n",
        "trainer_best = Trainer(\n",
        "    model=model_best,\n",
        "    args=training_args_best,\n",
        "    train_dataset=tokenized_dataset_final[\"train\"],\n",
        "    eval_dataset=tokenized_dataset_final[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Final training started...\")\n",
        "trainer_best.train()\n",
        "print(\"Final training completed!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final model with best hyperparameters...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retokenizing full dataset with best max_length: 256\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3869 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbfc4ec1b3c247008cde4aa9c66c98e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/829 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7e34e0b0bea415ea7e30d4bc4f91707"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/830 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cb5514369c44e04b411ad3f296e2b6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2776427982.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_best = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final training started...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='310' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [310/310 07:55, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.569817</td>\n",
              "      <td>0.302774</td>\n",
              "      <td>0.231293</td>\n",
              "      <td>0.290585</td>\n",
              "      <td>0.302774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.219848</td>\n",
              "      <td>0.558504</td>\n",
              "      <td>0.549816</td>\n",
              "      <td>0.584102</td>\n",
              "      <td>0.558504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.817149</td>\n",
              "      <td>0.722557</td>\n",
              "      <td>0.720344</td>\n",
              "      <td>0.729574</td>\n",
              "      <td>0.722557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.265200</td>\n",
              "      <td>0.717299</td>\n",
              "      <td>0.756333</td>\n",
              "      <td>0.756937</td>\n",
              "      <td>0.763220</td>\n",
              "      <td>0.756333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.265200</td>\n",
              "      <td>0.658588</td>\n",
              "      <td>0.784077</td>\n",
              "      <td>0.782876</td>\n",
              "      <td>0.790168</td>\n",
              "      <td>0.784077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.265200</td>\n",
              "      <td>0.639122</td>\n",
              "      <td>0.806996</td>\n",
              "      <td>0.809222</td>\n",
              "      <td>0.815893</td>\n",
              "      <td>0.806996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.400100</td>\n",
              "      <td>0.629611</td>\n",
              "      <td>0.809409</td>\n",
              "      <td>0.809271</td>\n",
              "      <td>0.809301</td>\n",
              "      <td>0.809409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.400100</td>\n",
              "      <td>0.651358</td>\n",
              "      <td>0.816647</td>\n",
              "      <td>0.817569</td>\n",
              "      <td>0.821635</td>\n",
              "      <td>0.816647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.400100</td>\n",
              "      <td>0.665128</td>\n",
              "      <td>0.819059</td>\n",
              "      <td>0.819181</td>\n",
              "      <td>0.820344</td>\n",
              "      <td>0.819059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.119900</td>\n",
              "      <td>0.670402</td>\n",
              "      <td>0.813028</td>\n",
              "      <td>0.813820</td>\n",
              "      <td>0.817003</td>\n",
              "      <td>0.813028</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "c3mtpIsqu9iy",
        "outputId": "4eef060c-c25d-4047-969c-058d0cedbd5b"
      },
      "source": [
        "print(\"Evaluating final model on test set...\")\n",
        "\n",
        "# Evaluate on the test set using the trainer with the best model loaded\n",
        "results_best = trainer_best.evaluate(tokenized_dataset_final[\"test\"])\n",
        "\n",
        "print(\"\\nFinal Model Test Results:\")\n",
        "for key, value in results_best.items():\n",
        "    print(f\"  {key}: {value:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating final model on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='104' max='104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [104/104 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Model Test Results:\n",
            "  eval_loss: 0.8044\n",
            "  eval_accuracy: 0.7675\n",
            "  eval_f1: 0.7676\n",
            "  eval_precision: 0.7704\n",
            "  eval_recall: 0.7675\n",
            "  eval_runtime: 2.5956\n",
            "  eval_samples_per_second: 319.7700\n",
            "  eval_steps_per_second: 40.0680\n",
            "  epoch: 10.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a14ab4b",
        "outputId": "9858e281-fe69-4bdd-aefd-835d7eaa1b87"
      },
      "source": [
        "print(\"\\nSample Predictions with Final Model:\")\n",
        "sample_texts = [\n",
        "    \"I've been feeling really down lately and can't seem to get out of bed. The world feels heavy and meaningless, and I've lost interest in everything I used to enjoy. It's a constant struggle to get through the day.\", # Depression\n",
        "    \"My anxiety is through the roof today, I can't stop worrying about everything. My heart is racing, my palms are sweaty, and I have this constant knot in my stomach. I'm scared to even leave the house.\", # Anxiety\n",
        "    \"I'm having trouble sleeping and my thoughts are racing constantly. One minute I'm incredibly energetic and feel like I can conquer the world, the next I'm in the depths of despair and can barely function. These mood swings are exhausting.\", # Bipolar disorder\n",
        "    \"I have mood swings that are out of control, one minute I'm fine, the next I'm इरिटेटेड. I struggle to maintain stable relationships because I'm so afraid of abandonment and often act impulsively, pushing people away.\", # Personality disorder (assuming this example fits)\n",
        "    \"I feel like people are always judging me and I can't form healthy relationships. Even in simple social situations, I feel intense scrutiny and self-doubt. It's hard to trust others, and I often isolate myself to avoid potential criticism or rejection.\", # Social Anxiety/Personality Disorder (adjusting to fit categories)\n",
        "    \"The pressure at work has been immense lately. Deadlines are piling up, and I feel completely overwhelmed. I'm constantly worried about not being good enough and it's starting to affect my sleep and overall well-being.\", # Stress\n",
        "    \"I had a traumatic experience a while back, and the memories keep flooding back. I have nightmares and flashbacks, and I'm constantly on edge. Loud noises or unexpected events trigger intense fear and panic.\", # Could potentially align with Stress or Anxiety depending on specifics, adding for variety\n",
        "    \"I feel so overwhelmed by everything in my life right now. School, work, relationships... it's all too much. I can't seem to catch a break and I'm constantly stressed about falling behind or disappointing someone.\" # Stress/Anxiety\n",
        "]\n",
        "\n",
        "# Ensure the model is on the correct device for prediction\n",
        "model_best.to(device)\n",
        "\n",
        "for text in sample_texts:\n",
        "    # Tokenize the input using the tokenizer associated with the best model's max_length\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=best_max_length, # Use the best max_length\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    # Move inputs to device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model_best(**inputs)\n",
        "        predictions = torch.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "        confidence = predictions[0][predicted_class].item()\n",
        "    # Get label name\n",
        "    predicted_label = label_mapping[predicted_class] # Access using integer key\n",
        "    print(f\"Text: {text[:70]}...\") # Display a longer snippet of text\n",
        "    print(f\"Prediction: {predicted_label} (confidence: {confidence:.3f})\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Report Final Results\n",
        "print(\"\\n--- Comprehensive Final Model Performance Report ---\")\n",
        "print(\"This report summarizes the hyperparameter tuning process and the performance of the final fine-tuned model.\")\n",
        "\n",
        "# Check if study object exists before trying to access its attributes\n",
        "if 'study' in locals() and study is not None:\n",
        "    print(\"\\nHyperparameter Tuning Summary (using Optuna):\")\n",
        "    print(f\"  Number of trials completed: {len(study.trials)}\")\n",
        "    print(f\"  Best validation metric (accuracy): {study.best_value:.4f}\")\n",
        "    print(\"  Best hyperparameters found:\")\n",
        "    for hp, value in study.best_params.items():\n",
        "        print(f\"    - {hp}: {value}\")\n",
        "else:\n",
        "    print(\"\\nHyperparameter tuning was not performed in this run.\")\n",
        "\n",
        "\n",
        "print(\"\\nFinal Model Test Set Metrics:\")\n",
        "print(\"The following metrics are based on the evaluation of the final model on the held-out test set.\")\n",
        "# Check if results_best object exists before trying to access its items\n",
        "if 'results_best' in locals() and results_best is not None:\n",
        "    for key, value in results_best.items():\n",
        "        if key.startswith(\"eval_\"):\n",
        "             print(f\"  {key.replace('eval_', '')}: {value:.4f}\")\n",
        "else:\n",
        "    print(\"Final model evaluation results are not available.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Predictions with Final Model:\n",
            "Text: I've been feeling really down lately and can't seem to get out of bed....\n",
            "Prediction: Depression (confidence: 0.954)\n",
            "----------------------------------------\n",
            "Text: My anxiety is through the roof today, I can't stop worrying about ever...\n",
            "Prediction: Anxiety (confidence: 0.982)\n",
            "----------------------------------------\n",
            "Text: I'm having trouble sleeping and my thoughts are racing constantly. One...\n",
            "Prediction: Bipolar disorder (confidence: 0.610)\n",
            "----------------------------------------\n",
            "Text: I have mood swings that are out of control, one minute I'm fine, the n...\n",
            "Prediction: Depression (confidence: 0.703)\n",
            "----------------------------------------\n",
            "Text: I feel like people are always judging me and I can't form healthy rela...\n",
            "Prediction: Personality disorder (confidence: 0.974)\n",
            "----------------------------------------\n",
            "Text: The pressure at work has been immense lately. Deadlines are piling up,...\n",
            "Prediction: Anxiety (confidence: 0.650)\n",
            "----------------------------------------\n",
            "Text: I had a traumatic experience a while back, and the memories keep flood...\n",
            "Prediction: Anxiety (confidence: 0.977)\n",
            "----------------------------------------\n",
            "Text: I feel so overwhelmed by everything in my life right now. School, work...\n",
            "Prediction: Stress (confidence: 0.969)\n",
            "----------------------------------------\n",
            "\n",
            "--- Comprehensive Final Model Performance Report ---\n",
            "This report summarizes the hyperparameter tuning process and the performance of the final fine-tuned model.\n",
            "\n",
            "Hyperparameter Tuning Summary (using Optuna):\n",
            "  Number of trials completed: 5\n",
            "  Best validation metric (accuracy): 0.8118\n",
            "  Best hyperparameters found:\n",
            "    - learning_rate: 4.498868200475396e-05\n",
            "    - per_device_train_batch_size: 32\n",
            "    - num_train_epochs: 10\n",
            "    - weight_decay: 0.1\n",
            "    - warmup_steps: 100\n",
            "    - gradient_accumulation_steps: 4\n",
            "    - max_length: 256\n",
            "\n",
            "Final Model Test Set Metrics:\n",
            "The following metrics are based on the evaluation of the final model on the held-out test set.\n",
            "  loss: 0.8044\n",
            "  accuracy: 0.7675\n",
            "  f1: 0.7676\n",
            "  precision: 0.7704\n",
            "  recall: 0.7675\n",
            "  runtime: 2.3134\n",
            "  samples_per_second: 358.7740\n",
            "  steps_per_second: 44.9550\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
